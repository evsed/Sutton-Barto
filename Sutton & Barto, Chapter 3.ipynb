{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Exercises for Chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1. <br>\n",
    "__The game of chess__. States: specific position of pieces on the board. Actions: moves. Rewards: +1 if the sequence of moves leads to a win, 0.5 if it leads to a draw, and 0 if it leads to a loss; any intermediate action that does not lead to a definitive mating combination gets reward of 0. <br>\n",
    "__Robot tennis player__. States: a vector that codes various game-related elements of the environment (position of the opponent on the court, rate of ball's revolutions, ball's speed etc.). Actions: a vector of several elements such as type of a shot (forehand, backhand, dropshot, slice etc.), strength of the shot, direction of movement, speed of movement, direction of a shot etc. Reward: + 1 if the point is won, 0 if the point is lost; under more sophisticated rules (e.g., robot going off-charge counts as a loss) +1 can be awarded only for the win of the entire match. <br>\n",
    "__Emotion Recognition Software__. States: a vector of pixels that together represent the picture of a human's face. Actions: a vector of weights (must sum to 1) representing proportion of each emotion corresponding to a specific image (real-life emotions are often mixtures of \"primitive\" types). Rewards: sum of absolute differences of an actual emotion weights (e.g., labeled by human coders) from predicted emotion weights. It's important to note that this example fits more into supervised learning paradigm because future states do not depend on agent's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2. <br>\n",
    "As example above demonstrates, the scope of applicability of MDP is quite large since even supervised machine learning tasks can be re-formulated as MDP-type problems. However, unsupervised learning tasks cannot be re-formulated as MDP-type problems. These tasks are certainly goal-directed, with finding underlying patterns in the data as the goal, but, unlike supervised learning tasks, unsupervised learning tasks cannot be re-formulated as learning-from-experience problems. Instead, it is often up to the algorithm designer to decide what constitutes a data pattern at the first place and what loss function best captures these patterns. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.3.\n",
    "It largely depends on the problem at hand. One scenario is non-deterministic route planning problem. The goal is to minimize time travelled between two points, and the road map is known to the agent, but the speed at different parts of the map is dynamically changing (e.g., due to traffic jams). Traffic jams are non-deterministic in a sense that the agent only has an imperfect estimate of whether it may happen at a certain point in the future. In this problem the actions are best defined in terms of where to drive. <br>\n",
    "On the other hand, if we want to program self-driving car, probably defining actions in terms of acceleration, breaks, and steering wheel is the best way to go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.4. <br>\n",
    "$\\mathbb{E}[R_{t+1}] = \\sum_{a \\in A}\\pi(a)\\sum_{r \\in R}r\\sum_{s \\in S}p(s, r | S_t, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3.5. <br>\n",
    "This table will be the same as presented in the text except for the cases where $p(s'|s,a) = 0$. This comes from the fact that in this example rewards are deterministic given $s', s, a$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.6. <br>\n",
    "Additional equation for terminal states: $\\sum_{s' \\in S}p(s',r|s,a) = 1, \\forall s \\in S^+/S, r = 0, a = \\emptyset$. Equation for non-terminal states is the same. We can also add condition: $p(s',r|s,a) = \\frac{1}{|S|}, \\forall s \\in S^+/S, r = 0, a = \\emptyset$ where $|S|$ denotes the number of non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.7. <br>\n",
    "Suppose at a certain learning episode failure occured after $K$ steps. Then at time $t = 0$ the return is $-\\gamma^K$, at time $t = 1$ the return  is $-\\gamma^{K-1}$ etc. So, for arbitrary $t$ and arbitrary learning episode $L$ the return is $G_{t}(L) = -\\gamma^{K(L) - t}$. For continuing tasks, the returns are different because multiple failures are allowed to happen in the future. Define $\\{F\\} =\\{f_1, f_2, f_3 ... f_n\\}$ as the monotone sequence of failure times, each counted from $t = 0$. Then the return can be defined as $G_t = \\sum_{f_i \\in F, \\ f_i>t}-\\gamma^{f_i - t}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.8. <br>\n",
    "The problem is in the definition of rewards. Defined this way, there is no difference for the agent if the exit is found after 10 steps or after 100 steps since both episodes give the same reward of +1. Better formulation would give the robot the negative reward each time it takes a step in the maze, and the positive reward once the escape had been found (0 reward would have worked as well since the task for the robot now is to minimize the number of steps it takes in the maze). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.9. <br>\n",
    "$G_4 = 2, G_3 = 3 + 0.5*2 = 4, G_2 = 6 + 0.5*4 = 8, G_1 = 2 + 0.5*8 = 6, G_0 = -1 + 0.5*6 = 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.10. <br>\n",
    "$G_0 = 2 + \\gamma*G_1 = 2 + 0.9*(\\frac{7}{0.1}) = 65$; $G_1 = 70$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.11. <br>\n",
    "$G_t = \\sum_{k = 0}^{K}\\gamma^k$. <br>\n",
    "$\\gamma G_t = \\sum_{k = 0}^{K}\\gamma^{k+1}$. <br>\n",
    "Substracting bottom equation from the top one we get: <br>\n",
    "$G_t(1 - \\gamma) = 1 - \\gamma^{K+1} \\implies G_t = \\frac{1 - \\gamma^{K+1}}{1 - \\gamma}$. Taking the limit $K \\to \\infty$ gives (3.10). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.12.\n",
    "$v_{\\pi}(s) = 0.25*(0 + 0.9*0.7 + 0 + 0.9*2.3 + 0 + 0.9*0.4 + 0 + 0.9*(-0.4)) = 0.25*(0.63 + 2.07) = 0.675 = 0.7$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.13.  <br>\n",
    "$$q_\\pi(s,a) = \\mathbb{E_\\pi}[G_t|S_t = s, A_t = a] $$\n",
    "$$= \\mathbb{E_\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t = s, A_t = a] $$\n",
    "$$= \\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)[r + \\gamma\\mathbb{E_\\pi}[G_{t+1}|S_{t+1} = s']] $$\n",
    "$$= \\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)[r + \\gamma\\sum_{a \\in A}\\pi(a|s')q(s',a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.14. <br>\n",
    "$$G_t' = \\sum_{k = 0}^{\\infty}\\gamma^k(R_{t+k+1} + c)$$ \n",
    "$$ = \\sum_{k = 0}^{\\infty}\\gamma^kR_{t+k+1} + \\sum_{k = 0}^{\\infty}\\gamma^kc$$\n",
    "$$ = \\sum_{k = 0}^{\\infty}\\gamma^kR_{t+k+1} + \\frac{c}{1 - \\gamma}$$. <br>\n",
    "$$v_{\\pi}(s)' = \\mathbb{E_\\pi}[G_{t}'|S_t = s]$$\n"   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.15. <br>\n",
    "$G_t' = \\sum_{k = 0}^{T}\\gamma^k(R_{t+k+1} + c) = \\sum_{k = 0}^{T}\\gamma^kR_{t+k+1} + c\\sum_{k = 0}^{T}\\gamma^k$. <br>\n",
    "$v_{\\pi}(s)' = \\mathbb{E_\\pi}[\\sum_{k = 0}^{T}\\gamma^kR_{t+k+1} + c\\sum_{k = 0}^{T}\\gamma^k|S_t = s]  $ <br>\n",
    "$=\\mathbb{E_\\pi}[\\sum_{k = 0}^{T}\\gamma^kR_{t+k+1}|S_t = s] + c\\mathbb{E_\\pi}[\\sum_{k = 0}^{T}\\gamma^k|S_t = s]$. <br> \n",
    "Therefore, in expectation relative values still remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.16. <br>\n",
    "$v_{\\pi}(s) = \\mathbb{E}_\\pi[q_{\\pi}(s,a)|S_t = s] = \\sum_{a \\in A}\\pi(a|s)q_{\\pi}(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.17. <br>\n",
    "$q_{\\pi}(s,a) = \\mathbb{E}_{s',r}[R_{t+1} + v_{\\pi}(s')|S_t = s, A_t = a] = \\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existence of Optimal Policy. <br>\n",
    "Existence of at least one optimal policy follows from the Extreme Value Theorem. The space of policies is compact, and value functions are continuous with respect to policies, so there is at least one policy that maximizes value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.18. <br>\n",
    "For any location outside the green, it is better to use the driver, so $v_*(s \\neq green) = q_*(s, driver)$. On the green, $v_*(s = green) = q_*(s, putter)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.19. <br>\n",
    "Let's denote states as $0, 1, 2, 3, 4, 5, 6$ by the number of putter strikes required to put the ball into a hole. <br>\n",
    "For any $s>2$, $q_*(s, putter) = -1 + \\gamma q_*(s-1, driver)$, so we get negative reward from hitting a ball with a putter and discounted reward from using a driver on the next strike because this is the optimal policy. For $s = 2$, we have $q_*(2, putter) = -1 + \\gamma q_*(1, putter)$ because the first strike gets us on the green, and we can now put the ball in one strike. For $s = 1$, we have $q_*(1, putter) = -1$. We can calculate all state-action values numerically backwards from $q_*(1, putter)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3.20. <br>\n",
    "$$q_*(h, s) = \\alpha[r_s + \\gamma\\max_{a'}q_*(h, a')] + (1 - \\alpha)[r_s + \\gamma\\max_{a'}q_*(l, a')] = r_s + \\gamma[\\alpha\\max_{a'}q_*(h, a') + (1 - \\alpha)\\max_{a'}q_*(l, a')]$$\n",
    "$$q_*(h, w) = r_w + \\gamma\\max_{a'}q_*(h, a')$$\n",
    "$$q_*(l, s) = \\beta[r_s + \\gamma\\max_{a'}q_*(l, a')] + (1 - \\beta)[-3 + \\gamma\\max_{a'}q_*(h, a')] = \\beta r_s - 3(1 - \\beta) + \\gamma[\\beta\\max_{a'}q_*(l, a') + (1 - \\beta)\\max_{a'}q_*(h, a')]$$\n",
    "$$q_*(l, w) = r_w + \\gamma\\max_{a'}q_*(l, a')$$\n",
    "$$q_*(l, re) = \\gamma\\max_{a'}q_*(h, a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.21. <br>\n",
    "$10 + 0.9*0 + 0.81*17.8 = 24.418$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.22. <br>\n",
    "For $\\gamma = 0$, we have left as optimal policy. <br>\n",
    "For $\\gamma = 0.9$, we need to compate values of initial state under two alternative policies. If the policy is to follow left, we have the system of equations: $$v_l(s_1) = 1 + 0.9*v_l(s_2)$$ $$v_l(s_2) = 0 + 0.9*v_l(s_1)$$. Solving this system gives $v_l(s_1) = 100/19$ - the value of the initial state after following action left. If the policy is to follow right, we have the following of equations: $$v_r(s_1) = 0 + 0.9*v_r(s_2)$$ $$v_r(s_2) = 2 + 0.9*v_r(s_1)$$. The value of the initial state under this policy is $180/19$, so following right is the optimal policy. <br>\n",
    "For $\\gamma = 0.5$, equations are the same except for 0.9 discount factor now becomes 0.5. Solving for left gives $v_l(s_1) = 4/3$; solving for right gives $v_r(s_1) = 4/3$. Thefore, both policies are optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.23. <br>\n",
    "$$v_*(s) = \\max_{a}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.24. <br>\n",
    "$$q_*(s,a) = \\sum_{r,s'}p(s',r|s,a)[r + \\gamma v_*(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.25. <br>\n",
    "$$\\pi_*(s) = \\text{arg}\\max_{a}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.26. <br>\n",
    "$$\\pi_*(s) = \\text{arg}\\max_{a}\\sum_{r,s'}p(s',r|s,a)[r + \\gamma v_*(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
